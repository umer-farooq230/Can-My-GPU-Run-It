<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"> 
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Can My GPU Run This AI Model? | Free LLM VRAM Calculator</title>

  <meta name="description" content="Instantly check if your GPU and RAM can run popular AI models like Llama 3, Mistral, Mixtral, and 70B locally. Accurate VRAM, KV cache, and memory compatibility calculator for local LLM inference.">
  <meta name="robots" content="index, follow">

  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      background: #fff;
      color: #000;
      padding: 20px;
      max-width: 900px;
      margin: 0 auto;
      line-height: 1.6;
    }

    header {
      margin-bottom: 40px;
      border-bottom: 2px solid #000;
      padding-bottom: 20px;
    }

    h1 {
      font-size: 32px;
      margin-bottom: 10px;
    }

    h2 {
      font-size: 22px;
      margin: 30px 0 15px 0;
    }

    .subtitle {
      color: #444;
      font-size: 16px;
      max-width: 700px;
    }

    .input-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin-bottom: 20px;
    }

    .input-section {
      margin-bottom: 20px;
    }

    label {
      display: block;
      font-weight: 600;
      margin-bottom: 8px;
      font-size: 14px;
    }

    select, input[type="number"], input[type="range"] {
      width: 100%;
      padding: 10px;
      font-size: 16px;
      border: 2px solid #000;
    }

    input[type="range"] {
      appearance: none;
      -webkit-appearance: none;
      height: 8px;
      background: #ddd;
      border: none;
      padding: 0;
    }

    input[type="range"]::-webkit-slider-thumb {
      -webkit-appearance: none;
      width: 20px;
      height: 20px;
      background: #000;
      cursor: pointer;
      border-radius: 50%;
    }

    button {
      background: #000;
      color: #fff;
      border: none;
      padding: 15px;
      font-size: 18px;
      font-weight: 600;
      cursor: pointer;
      width: 100%;
    }

    button:hover {
      background: #333;
    }

    #results {
      display: none;
      margin-top: 40px;
    }

    .note {
      margin-top: 40px;
      padding: 15px;
      background: #f5f5f5;
      font-size: 14px;
      border-left: 4px solid #000;
    }

    .seo-section {
      margin-top: 40px;
      font-size: 15px;
      color: #222;
    }

    .seo-section p {
      margin-bottom: 15px;
    }

    @media (max-width: 700px) {
      .input-grid {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>

<body>

  <header>
    <h1>Can My GPU Run This AI Model?</h1>
    <p class="subtitle">
      Instantly check if your GPU and RAM can run popular AI models locally. 
      Includes VRAM usage, KV cache estimation, and quantization-aware memory calculations.
    </p>
  </header>

  <main>

    <div class="input-grid">
      <div class="input-section">
        <label for="gpuSelect">Select Your GPU</label>
        <select id="gpuSelect">
          <option value="">-- Choose a GPU --</option>
        </select>
      </div>

      <div class="input-section">
        <label for="ramInput">System RAM (GB)</label>
        <input type="number" id="ramInput" min="4" max="256" step="4" value="16">
      </div>

      <div class="input-section">
        <label for="quantSelect">Quantization</label>
        <select id="quantSelect"></select>
      </div>

      <div class="input-section">
        <label for="context">Context Length: <span id="contextDisplay">2048</span> tokens</label>
        <input type="range" id="context" min="512" max="8192" step="512" value="2048">
      </div>
    </div>

    <button onclick="checkCompatibility()">Check Compatibility</button>

    <div id="results"></div>

    <div class="note">
      <strong>About the Calculations:</strong><br>
      Estimates include model weights, KV cache memory (which scales with context length),
      and runtime overhead.
      <br><br>
      ðŸŸ¢ <strong>SAFE</strong> = â‰¤85% GPU VRAM usage<br>
      ðŸŸ¡ <strong>TIGHT</strong> = Up to 100% GPU VRAM or requires RAM offload<br>
      ðŸ”´ <strong>NOT SUPPORTED</strong> = Exceeds available GPU + system RAM
    </div>

    <section class="seo-section">
      <h2>How It Works</h2>
      <p>
        Running large language models (LLMs) locally requires enough GPU VRAM 
        to store model weights and key-value (KV) cache memory. The required 
        memory increases with larger models and longer context lengths.
      </p>
      <p>
        This calculator estimates total GPU memory usage based on model size, 
        quantization level (4-bit, 8-bit, or FP16), and context length. 
        It helps determine whether a model runs entirely on GPU, requires system RAM offloading, 
        or exceeds your hardware limits.
      </p>
      <p>
        Supported models include Llama 3 (8B, 70B), Mistral 7B, Mixtral, 
        and other popular open-source LLMs used for local inference.
      </p>
    </section>

  </main>

  <script src="gpus.js"></script>
  <script src="models.js"></script>
  <script src="logic.js"></script>

  <script>
    const contextSlider = document.getElementById("context");
    const contextDisplay = document.getElementById("contextDisplay");

    contextSlider.addEventListener("input", () => {
      contextDisplay.textContent = contextSlider.value;
    });
  </script>

</body>
</html>
